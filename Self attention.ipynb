{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e369c7",
   "metadata": {},
   "source": [
    "# Task 2: Self-attention\n",
    "\n",
    "This was extremely difficult to implement since I did not know the theory behind self-attention, and it was fiendishly difficult trying to get layer names out of it. I went through many different libraries such as vit-pytorch and tried custom implementation of Vision Transformers, but eventually I gave up on the task because getting individual layer names out of it was extremely difficult (this is a [known problem](https://github.com/lukemelas/PyTorch-Pretrained-ViT/issues/19) in vit-pytorch) and a custom implementation of self-attention in the CNN led to the same issues - namely, that PyTorch layers are hard to index and implement, and it was beyond me. Therefore, I implemented self-attention in the last layer and only used it for visualization. Maybe I have misunderstood the task and done what was required (as I have implemented self-attention in the algorithm and trained the network again), but as I was approaching the deadline I implemented it this way. If I have made a mistake, I accept it.\n",
    "\n",
    "The first part of the code remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1000)\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available\")\n",
    "\n",
    "# Load the Galaxy10 dataset from an h5 file\n",
    "def load_galaxy10_data(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        images = f['images'][:]\n",
    "        labels = f['ans'][:]\n",
    "    return images, labels\n",
    "\n",
    "# Define a custom dataset class\n",
    "class Galaxy10Dataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc65ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple self-attention layer\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])  # (N, heads, query_len, key_len)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.nn.functional.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)  # (N, heads, query_len, key_len)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )  # (N, query_len, heads * head_dim)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e30bd",
   "metadata": {},
   "source": [
    "This code defines a self-attention layer. Self-attention layers are used to capture relationships between different elements in a sequence, such as words in a sentence.\n",
    "\n",
    "I have defined a custom PyTorch module named SelfAttention that inherits from nn.Module. This module represents a self-attention layer with customizable embedding size (embed_size) and the number of attention \"heads\" (heads).\n",
    "\n",
    "In the constructor __init__, the following attributes are defined:\n",
    "        embed_size: The size of the input embeddings.\n",
    "        heads: The number of attention heads to split the input into.\n",
    "        head_dim: The dimension of each attention head, calculated as embed_size // heads. This dimension determines how the input is split into multiple heads.\n",
    "\n",
    "The constructor includes an assert statement to ensure that the embed_size is divisible by the number of heads. This condition is required for the subsequent calculations to work correctly.\n",
    "\n",
    "Four linear (fully connected) layers are defined using nn.Linear. These layers are used to project the input embeddings into different spaces:\n",
    "        self.values: Projects the values for the attention mechanism.\n",
    "        self.keys: Projects the keys for the attention mechanism.\n",
    "        self.queries: Projects the queries for the attention mechanism.\n",
    "        self.fc_out: Combines the results from multiple attention heads into the final output.\n",
    "\n",
    "In the forward method, the self-attention mechanism is implemented:\n",
    "        The input tensors values, keys, and query are reshaped to split them into multiple heads using reshape. This allows each attention head to focus on different aspects of the input sequence. The projections for values, keys, and queries are obtained by passing them through their respective linear layers. Scaled dot-product attention is performed using torch.einsum, which calculates the attention scores (or energy) between queries and keys. If a mask is provided (used for tasks like sequence padding), it is applied to the energy scores to prevent attending to padded elements. The attention scores are normalized using the softmax function to obtain the attention weights. The final output is calculated by multiplying the attention weights with the values and then reshaping the result. Finally, the output is passed through the self.fc_out linear layer to combine the results from all the attention heads into a single tensor, which is then returned as the output of the self-attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9db3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and split it into train, validation, and test sets\n",
    "images, labels = load_galaxy10_data('../Galaxy10.h5')\n",
    "\n",
    "# Define data augmentation and transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(images, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = Galaxy10Dataset(X_train, y_train, transform=transform)\n",
    "val_dataset = Galaxy10Dataset(X_val, y_val, transform=transform)\n",
    "test_dataset = Galaxy10Dataset(X_test, y_test, transform=transform)\n",
    "\n",
    "# Define an Optuna objective function to optimize hyperparameters\n",
    "def objective(trial):\n",
    "    print(f\"Trial {trial.number}: Hyperparameter tuning in progress...\")\n",
    "    # Hyperparameter search space\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [4, 8, 16, 32, 64])\n",
    "\n",
    "    # Create data loaders with the chosen batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 10)  # 10 output classes for Galaxy10\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 7\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Log file for training output\n",
    "    log_file = open('training_log.txt', 'w')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', ncols=100):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Calculate average training loss for this epoch\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                val_outputs = model(inputs)\n",
    "                _, val_preds = torch.max(val_outputs, 1)\n",
    "                val_predictions.extend(val_preds.cpu().numpy())\n",
    "                val_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(val_targets, val_predictions)\n",
    "        log_message = f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}'\n",
    "        print(log_message)\n",
    "        log_file.write(log_message + '\\n')\n",
    "\n",
    "        # Save the model if it's the best so far\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    log_file.close()\n",
    "    return -best_accuracy  # Optuna minimizes the objective, so we negate accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7d916",
   "metadata": {},
   "source": [
    "The Optuna optimization remains mostly the same, but I reduced the number of epochs to make it run faster. Previous self-attention implementattions ran very slowly so I decided to reduce the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2703d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna hyperparameter optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "best_weight_decay = best_params['weight_decay']\n",
    "best_batch_size = best_params['batch_size']\n",
    "\n",
    "print(f'Best Learning Rate: {best_learning_rate:.6f}')\n",
    "print(f'Best Weight Decay: {best_weight_decay:.6f}')\n",
    "print(f'Best Batch Size: {best_batch_size}')\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = best_model.fc.in_features\n",
    "best_model.fc = nn.Linear(num_ftrs, 10)\n",
    "best_model.to(device)\n",
    "best_optimizer = optim.Adam(best_model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)\n",
    "\n",
    "# Create data loaders with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_batch_size, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_batch_size, num_workers=4)\n",
    "\n",
    "num_epochs = 25\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Log file for training output\n",
    "log_file = open('training_log.txt', 'a')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('in the best model training loop right now')\n",
    "    best_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', ncols=100):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        best_optimizer.zero_grad()\n",
    "        outputs = best_model(inputs)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        best_optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate average training loss for this epoch\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    best_model.eval()\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            val_outputs = best_model(inputs)\n",
    "            _, val_preds = torch.max(val_outputs, 1)\n",
    "            val_predictions.extend(val_preds.cpu().numpy())\n",
    "            val_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_targets, val_predictions)\n",
    "    log_message = f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}'\n",
    "    print(log_message)\n",
    "    log_file.write(log_message + '\\n')\n",
    "\n",
    "    # Save the model if it's the best so far\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(best_model.state_dict(), 'best_model.pth')\n",
    "\n",
    "log_file.close()\n",
    "\n",
    "# Load the best model\n",
    "best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "best_model.eval()\n",
    "\n",
    "# Testing\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        test_outputs = best_model(inputs)\n",
    "        _, test_preds = torch.max(test_outputs, 1)\n",
    "        test_predictions.extend(test_preds.cpu().numpy())\n",
    "        test_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "test_accuracy = accuracy_score(test_targets, test_predictions)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Classification report\n",
    "class_names = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6', 'Class 7', 'Class 8', 'Class 9']\n",
    "classification_rep = classification_report(test_targets, test_predictions, target_names=class_names)\n",
    "\n",
    "# Save outputs to a file\n",
    "with open('output.txt', 'w') as output_file:\n",
    "    output_file.write(\"Test Accuracy: {:.4f}\\n\".format(test_accuracy))\n",
    "    output_file.write(\"\\nClassification Report:\\n\")\n",
    "    output_file.write(classification_rep)\n",
    "\n",
    "print(\"Outputs saved to 'output.txt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840a91fd",
   "metadata": {},
   "source": [
    "This is the same code. Now we plot attention maps onto an image. This was particularly difficult and is the reason I took so long to do what should have been a very straightforward task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25275132",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c9cbd7",
   "metadata": {},
   "source": [
    "For some reason, everything reset so this made the code work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d76670c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5py' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load an image on which you want to overlay the attention map\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m f\u001b[38;5;241m=\u001b[39m\u001b[43mh5py\u001b[49m\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Galaxy10.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m image_to_overlay_attention \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Convert the image to a tensor and apply transformations\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h5py' is not defined"
     ]
    }
   ],
   "source": [
    "# Load an image on which you want to overlay the attention map\n",
    "f=h5py.File('../Galaxy10.h5','r')\n",
    "image_to_overlay_attention = f['images'][0]\n",
    "\n",
    "# Convert the image to a tensor and apply transformations\n",
    "image_tensor = transform(image_to_overlay_attention).unsqueeze(0).to(device)  # Move image_tensor to the GPU\n",
    "\n",
    "# Apply the self-attention layer to the image\n",
    "self_attention = SelfAttention(embed_size=3, heads=1).to(device)  # Move self_attention to the GPU\n",
    "\n",
    "# Reshape the image_tensor to match the expected shape (batch_size, sequence_length, embed_size)\n",
    "batch_size, channels, height, width = image_tensor.shape\n",
    "sequence_length = height * width  # Assuming you treat each pixel as a separate \"sequence\"\n",
    "image_tensor = image_tensor.view(batch_size, sequence_length, channels)\n",
    "\n",
    "# Compute the attention map\n",
    "attention_map = self_attention(image_tensor, image_tensor, image_tensor, mask=None)\n",
    "\n",
    "# Normalize the attention map for visualization\n",
    "normalized_attention_map = torch.nn.functional.normalize(attention_map, dim=2, p=1)\n",
    "\n",
    "# Convert the attention map to an image\n",
    "attention_map_image = normalized_attention_map.squeeze().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64ed62",
   "metadata": {},
   "source": [
    "This implements self-attention in the last layer and uses it for visualization. Note that I have used the scaled dot-product form of self-attention as it was the easiest for me to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9723ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay the attention map on the original image\n",
    "def overlay_attention(original_image, attention_map):\n",
    "    # Resize the attention map to match the original image size\n",
    "    attention_map = cv2.resize(attention_map, (original_image.shape[1], original_image.shape[0]))\n",
    "\n",
    "    # Apply a colormap to the attention map (you can choose your preferred colormap)\n",
    "    colormap = cv2.applyColorMap(np.uint8(255 * attention_map), cv2.COLORMAP_JET)\n",
    "\n",
    "    # Blend the attention map with the original image\n",
    "    overlay_image = cv2.addWeighted(original_image, 0.7, colormap, 0.3, 0)\n",
    "\n",
    "    return overlay_image\n",
    "\n",
    "# Overlay the attention map on the original image\n",
    "overlay_image = overlay_attention(image_to_overlay_attention, attention_map_image)\n",
    "\n",
    "# Save the overlay image as an output\n",
    "cv2.imwrite('attention_overlay_image.jpg', overlay_image)\n",
    "\n",
    "print(\"Attention overlay image saved.\")\n",
    "print(\"Reached the end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d32df1",
   "metadata": {},
   "source": [
    "This prints and shows the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28eb90d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread('selfat/attention_overlay_image.jpg')\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5174f26",
   "metadata": {},
   "source": [
    "It seems like the corners of the galaxy are used as a main classifier, which makes sense, as Galaxy10 galaxies are classified as spiral, barred spiral, etc depending on their shape and this is the easiest way to tell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
